{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3-D Density Kernel Estimation for Counting in Microscopy Image Volumes Using 3-D Image Filters and Random Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### By Dominic Waithe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From journal paper:  \n",
    "Waithe, Dominic, et al. \"3-D Density Kernel Estimation for Counting in Microscopy Image Volumes Using 3-D Image Filters and Random Decision Trees.\" European Conference on Computer Vision. Springer International Publishing, 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The Functions and dependencies used for the 3D Density Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "#Dependencies.\n",
    "import vigra\n",
    "import sys\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n",
    "from scipy.ndimage import measurements, center_of_mass, filters\n",
    "from scipy.ndimage import gaussian_gradient_magnitude, gaussian_laplace, gaussian_filter\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import time\n",
    "import tifffile as tif_fn\n",
    "\n",
    "\n",
    "def evaluate_forest_3D(par_obj,model_num,count=None):\n",
    "    \"\"\"Evaluates the forest at a particular slice\"\"\"\n",
    "    mimg_lin = np.reshape(par_obj.feat_arr[count], \\\n",
    "                          (par_obj.height * par_obj.width, par_obj.feat_arr[count].shape[2]))\n",
    "    linPred = par_obj.RF[model_num].predict(mimg_lin)/4.0\n",
    "    par_obj.pred_arr[count] = linPred.reshape(par_obj.height, par_obj.width)\n",
    "    maxPred = np.max(linPred)\n",
    "    sum_pred =np.sum(linPred/255)\n",
    "    par_obj.sum_pred[count] = sum_pred\n",
    "\n",
    "def local_shape_features_2D(im,scaleStart):\n",
    "    \"\"\"2-D Image Shape features\"\"\"\n",
    "    s = scaleStart\n",
    "    \n",
    "    imSizeC = im.shape[0]\n",
    "    imSizeR = im.shape[1]\n",
    "    f = np.zeros((imSizeC,imSizeR,21))\n",
    "    \n",
    "    st08 = vigra.filters.structureTensorEigenvalues(im,s*1,s*2)\n",
    "    st16 = vigra.filters.structureTensorEigenvalues(im,s*2,s*4)\n",
    "    st32 = vigra.filters.structureTensorEigenvalues(im,s*4,s*8)\n",
    "    st64 = vigra.filters.structureTensorEigenvalues(im,s*8,s*16)\n",
    "    st128 = vigra.filters.structureTensorEigenvalues(im,s*16,s*32)\n",
    "    \n",
    "    f[:,:, 0]  =  im\n",
    "    f[:,:, 1]  =  gaussian_gradient_magnitude(im, s)\n",
    "    f[:,:, 2]  =  st08[:,:,0]\n",
    "    f[:,:, 3]  =  st08[:,:,1]\n",
    "    f[:,:, 4]  =  gaussian_laplace(im, s )\n",
    "    f[:,:, 5]  =  gaussian_gradient_magnitude(im, s*2) \n",
    "    f[:,:, 6]  =  st16[:,:,0]\n",
    "    f[:,:, 7]  =  st16[:,:,1]\n",
    "    f[:,:, 8]  =  gaussian_laplace(im, s*2 )\n",
    "    f[:,:, 9]  =  gaussian_gradient_magnitude(im, s*4) \n",
    "    f[:,:, 10] =  st32[:,:,0]\n",
    "    f[:,:, 11] =  st32[:,:,1]\n",
    "    f[:,:, 12] =  gaussian_laplace(im, s*4 )\n",
    "    f[:,:, 13]  = gaussian_gradient_magnitude(im, s*8) \n",
    "    f[:,:, 14] =  st64[:,:,0]\n",
    "    f[:,:, 15] =  st64[:,:,1]\n",
    "    f[:,:, 16] =  gaussian_laplace(im, s*8 )\n",
    "    f[:,:, 17]  = gaussian_gradient_magnitude(im, s*16) \n",
    "    f[:,:, 18] =  st128[:,:,0]\n",
    "    f[:,:, 19] =  st128[:,:,1]\n",
    "    f[:,:, 20] =  gaussian_laplace(im, s*16 )\n",
    "    return f\n",
    "def local_shape_features_3D(input_s,scaleStart):\n",
    "    \"\"\"3-D Image Shape features\"\"\"\n",
    "    s = scaleStart\n",
    "    \n",
    "    imSizeZ = input_s.shape[0]\n",
    "    imSizeC = input_s.shape[1]\n",
    "    imSizeR = input_s.shape[2]\n",
    "    \n",
    "    \n",
    "    #If the input is small we need to zero-pad for the Eigenvalue determination.\n",
    "    if imSizeZ < 40:    \n",
    "        input_ss =  np.zeros((40,imSizeC,imSizeR))\n",
    "        input_ss[0:imSizeZ,:,:] = input_s\n",
    "    else:\n",
    "        input_ss = input_s\n",
    "        \n",
    "    f = np.zeros((imSizeZ,imSizeC,imSizeR,25))\n",
    "    \n",
    "    f[:,:,:,0] = np.array(input_s)\n",
    "    \n",
    "    f[:,:,:,1] = gaussian_gradient_magnitude(input_s, [(s/1),s,s])\n",
    "    f[:,:,:,2] = gaussian_laplace(input_s,[(s/1),s,s])\n",
    "    f[:,:,:,3] =  gaussian_filter(input_s,[(s/1),s,s])\n",
    "    fe = vigra.filters.structureTensorEigenvalues(input_ss.astype(np.float32),\\\n",
    "                                                  [(s/1),s,s],[(s/1)*2,s*2,s*2])\n",
    "    f[:,:,:,4] = fe[:,:,:,0]\n",
    "    f[:,:,:,5] = fe[:,:,:,1]\n",
    "    f[:,:,:,6] = fe[:,:,:,2]  \n",
    "    \n",
    "    f[:,:,:,7] = gaussian_gradient_magnitude(input_s, [(s/1)*2,s*2,s*2])\n",
    "    f[:,:,:,8] = gaussian_laplace(input_s,[(s/1)*2,s*2,s*2])\n",
    "    f[:,:,:,9] =  gaussian_filter(input_s,[(s/1)*2,s*2,s*2])\n",
    "    fe = vigra.filters.structureTensorEigenvalues(input_ss.astype(np.float32),\\\n",
    "                                                  [(s/1)*2,s*2,s*2],[(s/1)*4,s*4,s*4])\n",
    "    f[:,:,:,10] = fe[:,:,:,0]\n",
    "    f[:,:,:,11] = fe[:,:,:,1]\n",
    "    f[:,:,:,12] = fe[:,:,:,2]    \n",
    "    \n",
    "    f[:,:,:,13] = gaussian_gradient_magnitude(input_s, [(s/1)*4,s*4,s*4])\n",
    "    f[:,:,:,14] = gaussian_laplace(input_s,[(s/1)*4,s*4,s*4])\n",
    "    f[:,:,:,15] =  gaussian_filter(input_s,[(s/1)*4,s*4,s*4])\n",
    "    fe = vigra.filters.structureTensorEigenvalues(input_ss.astype(np.float32),\\\n",
    "                                                  [(s/1)*4,s*4,s*4],[(s/1)*8,s*8,s*8])\n",
    "    f[:,:,:,16] = fe[:,:,:,0]\n",
    "    f[:,:,:,17] = fe[:,:,:,1]\n",
    "    f[:,:,:,18] = fe[:,:,:,2]\n",
    "    \n",
    "    f[:,:,:,19] = gaussian_gradient_magnitude(input_s, s*8)\n",
    "    f[:,:,:,20] = gaussian_laplace(input_s,s*8)\n",
    "    f[:,:,:,21] = gaussian_filter(input_s,s*8)\n",
    "    fe = vigra.filters.structureTensorEigenvalues(input_ss.astype(np.float32),\\\n",
    "                                                  [(s/1)*8,s*8,s*8],[(s/1)*16,s*16,s*16])\n",
    "    f[:,:,:,22] = fe[:,:,:,0]\n",
    "    f[:,:,:,23] = fe[:,:,:,1]\n",
    "    f[:,:,:,24] = fe[:,:,:,2]\n",
    "    \n",
    "    out  = {}\n",
    "    for z in range(0,imSizeZ):\n",
    "        out[z] = f[z,:,:,:]\n",
    "    return out\n",
    "\n",
    "def update_training_samples_fn_ext(par_obj,model_num):\n",
    "    \"\"\"Collects the pixels which will be used for training sub-sampling as required.\"\"\"\n",
    "    region_size = 0\n",
    "    for b in range(0,par_obj.saved_ROI.__len__()):\n",
    "        rects = par_obj.saved_ROI[b]\n",
    "        region_size += rects[4]*rects[3]        \n",
    "    \n",
    "    calc_ratio = par_obj.limit_ratio_size\n",
    "    \n",
    "\n",
    "    for b in range(0,par_obj.saved_ROI.__len__()):\n",
    "\n",
    "        #Iterates through saved ROI.\n",
    "        rects = par_obj.saved_ROI[b]\n",
    "        img2load = rects[0]\n",
    "\n",
    "        #Finds and extracts the features and output density for the specific regions.\n",
    "        mImRegion = par_obj.feat_arr[rects[0]][rects[2]+1:rects[2]+rects[4],\\\n",
    "                                               rects[1]+1:rects[1]+rects[3],:]\n",
    "        denseRegion = par_obj.dense_array[rects[0]][rects[2]+1:rects[2]+rects[4],\\\n",
    "                                                    rects[1]+1:rects[1]+rects[3]]\n",
    "        #Find the linear form of the selected feature representation\n",
    "        mimg_lin = np.reshape(mImRegion, (mImRegion.shape[0]*mImRegion.shape[1],mImRegion.shape[2]))\n",
    "        #Find the linear form of the complementatory output region.\n",
    "        dense_lin = np.reshape(denseRegion, (denseRegion.shape[0]*denseRegion.shape[1]))\n",
    "        #Sample the input pixels sparsely or densely.\n",
    "        if(par_obj.limit_sample == True):\n",
    "            if(par_obj.limit_ratio == True):\n",
    "                par_obj.limit_size = round(mImRegion.shape[0]*mImRegion.shape[1]/calc_ratio,0)\n",
    "\n",
    "            #Randomly sample from input ROI or im a certain number (par_obj.limit_size) patches. With replacement.\n",
    "            indices =  np.random.choice(int(mImRegion.shape[0]*mImRegion.shape[1]),\\\n",
    "                                        size=int(par_obj.limit_size), replace=True, p=None)\n",
    "            #Add to feature vector and output vector.\n",
    "            par_obj.f_matrix.extend(mimg_lin[indices])\n",
    "            par_obj.o_patches.extend(dense_lin[indices]*4.0)\n",
    "        else:\n",
    "            #Add these to the end of the feature Matrix, input patches\n",
    "            par_obj.f_matrix.extend(mimg_lin)\n",
    "            #And the the output matrix, output patches\n",
    "            par_obj.o_patches.extend(dense_lin)\n",
    "        \n",
    "        \n",
    "    \n",
    "def im_dimen_fn(par_obj,file_array):\n",
    "    \"\"\"Function which loads in Tiff stack or single png file to assess dimensions.\"\"\"\n",
    "    prevExt = [] \n",
    "    prevBitDepth=[] \n",
    "    prevNumCH =[]\n",
    "    par_obj.numCH = 0\n",
    "    par_obj.total_time_pt = 0\n",
    "    for i in range(0,file_array.__len__()):\n",
    "            n = str(i)\n",
    "            imStr = str(file_array[i])\n",
    "            par_obj.file_ext = imStr.split(\".\")[-1]\n",
    "            if par_obj.file_ext == 'tif' or par_obj.file_ext == 'tiff':\n",
    "                par_obj.tiff_file = tif_fn.TiffFile(imStr)\n",
    "                meta = par_obj.tiff_file.series[0]\n",
    "                order = meta.axes\n",
    "                \n",
    "                #Reads parameters from TIFF file input.\n",
    "                for n,b in enumerate(order):\n",
    "                    if b == 'T':\n",
    "                        par_obj.total_time_pt = meta.shape[n]\n",
    "                    if b == 'I':\n",
    "                        par_obj.max_zslices = meta.shape[n]\n",
    "                    if b == 'Z':\n",
    "                        par_obj.max_zslices = meta.shape[n]\n",
    "                    if b == 'Y':\n",
    "                        par_obj.ori_height = meta.shape[n]\n",
    "                    if b == 'X':\n",
    "                        par_obj.ori_width = meta.shape[n]\n",
    "                    if b == 'S':\n",
    "                        par_obj.numCH = meta.shape[n]\n",
    "\n",
    "                par_obj.bitDepth = meta.dtype\n",
    "            else:\n",
    "                 statusText = 'Status: Image format not-recognised. Please choose either png or TIFF files.'\n",
    "                 return False, statusText\n",
    "\n",
    "    statusText= str(file_array.__len__())+' Files Loaded.'\n",
    "    return True, statusText\n",
    "def im_pred_inline_fn(par_obj,ch_to_import):\n",
    "    \"\"\"Accesses TIFF file. Calculates features to indices present in par_obj.left_2_calc\"\"\"\n",
    "    \n",
    "    \n",
    "    #Goes through the list of files.\n",
    "    for b in par_obj.left_2_calc:\n",
    "            #input string of file.\n",
    "            imStr = str(par_obj.file_array[b])\n",
    "            #Finds frames to be loaded for this file.\n",
    "            frames = par_obj.frames_2_load[b]\n",
    "            \n",
    "            \n",
    "            if par_obj.file_ext == 'tif' or par_obj.file_ext == 'tiff':\n",
    "            #Make sure we have a tiff image, necessary for the stacks.\n",
    "                if par_obj.features == '2D':\n",
    "                    #For the case when we are calculating 2-D filters.\n",
    "                    count = -1\n",
    "                    for i in frames:\n",
    "                        count = count+1\n",
    "                        keyframe = (par_obj.max_zslices*par_obj.time_pt)+ i\n",
    "                        if ch_to_import == 'default':\n",
    "                            input_i = par_obj.tiff_file.asarray()[keyframe,:,:]\n",
    "                        else:\n",
    "                            input_i = par_obj.tiff_file.asarray()[keyframe,ch_to_import,:,:]\n",
    "                        print 'Calculating Features for Image: ',(b+1),' Frame: ',(i+1)\n",
    "                        feat = local_shape_features_2D(input_i.astype(np.float32),par_obj.feature_scale)\n",
    "                        par_obj.feat_arr[count] = feat\n",
    "                        par_obj.num_of_feat = feat.shape[2]\n",
    "                elif par_obj.features == '3D':\n",
    "                    #For the case when we are calculating 3-D filters.\n",
    "                    if ch_to_import == 'default':\n",
    "                        input_s = par_obj.tiff_file.asarray()[:,:,:]\n",
    "                    else:\n",
    "                        input_s = par_obj.tiff_file.asarray()[:,ch_to_import,:,:]\n",
    "                    feat = local_shape_features_3D(input_s.astype(np.float32),par_obj.feature_scale)\n",
    "                    print 'Calculating Features for Image: ',(b+1)\n",
    "                    #export parameters and data\n",
    "                    par_obj.num_of_feat = 25\n",
    "                    par_obj.feat_arr = feat\n",
    "                \n",
    "    \n",
    "    return\n",
    "def process_imgs(par_obj,b,ch_to_import):\n",
    "    \"\"\"Imports ground-truth and input file. Calculates density represntation.\"\"\"\n",
    "    #We work on the full-size ground-truth file.\n",
    "    GT_obj = tif_fn.TiffFile(par_obj.gt_file_array[b])\n",
    "    #Convert to 3D array.\n",
    "    GT_mov = GT_obj.asarray()\n",
    "\n",
    "    ##### Create the HUMAN label data from Ground-truth data. #####\n",
    "    #Generate empty container for the density label representation.\n",
    "    par_obj.gt_sum[b] = np.sum(GT_mov)/255\n",
    "    #Very important the size of the gaussian kernel\n",
    "    sigma = par_obj.sigma;\n",
    "    #The output density image.\n",
    "    uncrop_dense_array = filters.gaussian_filter(GT_mov.astype(np.float32),\\\n",
    "                                                 sigma, order=0, output=None, mode='constant', cval=0.0)\n",
    "    par_obj.dense_array = uncrop_dense_array[par_obj.frames_2_load[b][0]:par_obj.frames_2_load[b][-1]+1,:,:]\n",
    "    #Make sure to add an image.\n",
    "    par_obj.left_2_calc = [b]\n",
    "    par_obj.gt_sum[b] = np.sum(GT_mov[par_obj.frames_2_load[b][0]:par_obj.frames_2_load[b][-1]+1,:,:])/255\n",
    "    par_obj.gt_dense[b] = np.sum(par_obj.dense_array)/255\n",
    "\n",
    "    ##### Calculates features for the given image.\n",
    "    par_obj.tiff_file = tif_fn.TiffFile(par_obj.file_array[b])\n",
    "    im_pred_inline_fn(par_obj,ch_to_import)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ParameterContainer():\n",
    "    def __init__(self):\n",
    "        self.file_array = []\n",
    "        self.gt_file_array = []\n",
    "        self.frames_2_load = {}\n",
    "        self.feature_scale = 0.8\n",
    "        self.p_size = 1\n",
    "        self.feat_arr = {}\n",
    "        self.features = '3D' #'2D' or '3D'\n",
    "        self.limit_sample = True\n",
    "        self.limit_ratio = True\n",
    "        self.limit_ratio_size = 200\n",
    "        self.max_depth = 20\n",
    "        self.min_samples_split = 20 \n",
    "        self.min_samples_leaf = 10  \n",
    "        self.max_features = 7\n",
    "        self.num_of_tree = 100\n",
    "        self.resize_factor = 1\n",
    "        self.gt_sum = {}\n",
    "        self.gt_dense ={}\n",
    "        self.f_matrix =[]\n",
    "        self.o_patches=[]\n",
    "        self.gt_sum = {}\n",
    "        self.gt_dense ={}\n",
    "\n",
    "tif_in_path = \"data/\" \n",
    "tif_out_path = \"out/\"\n",
    "csv_out_Path = \"out/\"\n",
    "\n",
    "\n",
    "for exp in [1]: #The experiment to be run.\n",
    "    for num_of_train in [8]: #The number of training images used in case.\n",
    "        for bc in range(0,10): #The number of times you want to repeat the experiment.\n",
    "            \n",
    "            #Where we store parameters for model and related.\n",
    "            par_obj = ParameterContainer()\n",
    "            \n",
    "\n",
    "            if exp == 1:\n",
    "                path = tif_in_path+'dataset1/'\n",
    "                num_of_im = 30 #Number of images in the dataset.\n",
    "                ss = ShuffleSplit(num_of_im, n_iter=1, test_size=15, train_size=num_of_train)\n",
    "                par_obj.sigma =  [float(13),float(12),float(12)] #z, x, y\n",
    "                ch_import_train = ch_import_test = 'default'\n",
    "            if exp == 2:\n",
    "                path = tif_in_path+'dataset2/'\n",
    "                num_of_im = 30 #Number of images in the dataset.\n",
    "                ss = ShuffleSplit(num_of_im, n_iter=1, test_size=15, train_size=num_of_train)\n",
    "                par_obj.sigma =  [float(10),float(6),float(6)] #z, x, y\n",
    "                ch_import_train = ch_import_test = 'default'\n",
    "            if exp == 3:\n",
    "                path = tif_in_path+'dataset3/'\n",
    "                num_of_im = 26 #Number of images in the dataset.\n",
    "                ss = ShuffleSplit(num_of_im, n_iter=1, test_size=15, train_size=num_of_train)\n",
    "                par_obj.sigma =  [float(4),float(15),float(15)] #z, x, y\n",
    "                ch_import_train = ch_import_test = 'default'\n",
    "            if exp == 4:\n",
    "                path = tif_in_path+'dataset4/'\n",
    "                par_obj.sigma =  [float(1.8),float(1.8),float(1.8)] #z, x, y\n",
    "                num_of_im = 10 #Number of images in the dataset.\n",
    "                ss = ShuffleSplit(num_of_im, n_iter=1, test_size=2, train_size=num_of_train)\n",
    "                ch_import_train = ch_import_test = 1\n",
    "               \n",
    "\n",
    "\n",
    "            for train_index, test_index in ss:\n",
    "                 print(\"%s %s\" % (train_index, test_index))\n",
    "            \n",
    "            #Load in data.\n",
    "            for i in range(0,num_of_im):\n",
    "                n = str(i).zfill(3)\n",
    "                par_obj.file_array.append(path+'/image-final_0'+n+'.tif')\n",
    "                par_obj.gt_file_array.append(path+'/image-dots_0'+n+'.tif')\n",
    "                im_dimen_fn(par_obj,[par_obj.file_array[-1]])\n",
    "                par_obj.frames_2_load[i] = np.arange(0,par_obj.max_zslices)\n",
    "                par_obj.height = par_obj.ori_height\n",
    "                par_obj.width = par_obj.ori_width\n",
    "            \n",
    "            print('Calculating features')\n",
    "            for b in train_index:\n",
    "\n",
    "                t1 = time.time()\n",
    "                process_imgs(par_obj,b,ch_import_train)\n",
    "                \n",
    "                t2 = time.time()\n",
    "                print \"Image_id\",b, 'time taken: ',t2-t1\n",
    "                #Need to initialise are region for each image, which encompasses the whole plane.\n",
    "                par_obj.saved_ROI = []\n",
    "                for i in range(0,par_obj.frames_2_load[b].__len__()):\n",
    "                    rects = (i,0,0,par_obj.width,par_obj.height)\n",
    "                    par_obj.saved_ROI.append(rects)\n",
    "\n",
    "\n",
    "                #Add the training data from this file.\n",
    "                update_training_samples_fn_ext(par_obj,0)\n",
    "\n",
    "            ##### Processes images by importing the image, the ground-truth image and calculates the features \n",
    "\n",
    "            par_obj.RF ={}\n",
    "            par_obj.RF[0] = ExtraTreesRegressor(par_obj.num_of_tree, max_depth=par_obj.max_depth,\\\n",
    "                                                min_samples_split=par_obj.min_samples_split,\\\n",
    "                                                min_samples_leaf=par_obj.min_samples_leaf, \\\n",
    "                                                max_features=par_obj.max_features, bootstrap=True, n_jobs=-1)\n",
    "\n",
    "\n",
    "            par_obj.pred_arr = {}\n",
    "            par_obj.sum_pred = {}\n",
    "\n",
    "            ######\n",
    "            ###### Learns the ensemble of decision trees\n",
    "            ######\n",
    "            \n",
    "            t3 = time.time()\n",
    "            \n",
    "            par_obj.RF[0].fit(par_obj.f_matrix, par_obj.o_patches)\n",
    "            \n",
    "            t4 = time.time()\n",
    "            \n",
    "            print 'Time to train',t4-t3\n",
    "            \n",
    "            ######\n",
    "            ###### Evaluation of images.\n",
    "            ######\n",
    "            \n",
    "            par_obj.final_prediction = {}\n",
    "            time_taken_to_calc_feat = []\n",
    "            time_taken_to_eval_trees = []\n",
    "            \n",
    "            print 'Calculating features for evaluation'\n",
    "            par_obj.the_score = {}\n",
    "            for b in test_index:\n",
    "                par_obj.sum_pred ={}\n",
    "\n",
    "                t1 = time.time()\n",
    "                process_imgs(par_obj,b,ch_import_test)\n",
    "                t2 = time.time()\n",
    "                \n",
    "                print 'Image_id: ',b,' time taken to calc features: ',t2-t1\n",
    "        \n",
    "                t1 = time.time()\n",
    "\n",
    "                #Evaluates each frame using learnt framework.\n",
    "                for it in range(0,par_obj.frames_2_load[b].__len__()):\n",
    "                    evaluate_forest_3D(par_obj,0,count=it)\n",
    "                    \n",
    "                t2 = time.time()\n",
    "                \n",
    "                print 'Image_id: ',b,' time taken to eval trees: ',t2-t1\n",
    "                \n",
    "                \n",
    "                par_obj.final_prediction[b] = 0\n",
    "                for c in par_obj.sum_pred:\n",
    "                    par_obj.final_prediction[b] += par_obj.sum_pred[c]\n",
    "                \n",
    "                \n",
    "                #Outputs Image density image\n",
    "                with tif_fn.TiffWriter(tif_out_path+'/exp_'+str(exp)+'_image-dense_0'+str(b)+'.tif',\\\n",
    "                                       bigtiff=True) as tif:\n",
    "                    for i in range(par_obj.sum_pred.__len__()):\n",
    "                        tif.save(par_obj.pred_arr[i], compress=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            ######\n",
    "            ###### Outputs the performance.\n",
    "            ######\n",
    "            per_err = []\n",
    "            for i,item in enumerate(test_index):\n",
    "                per_err.append((1-(abs(par_obj.gt_dense[item] - \\\n",
    "                                       par_obj.final_prediction[item])/par_obj.gt_dense[item]))*100)\n",
    "            print \"output accuracy\",np.average(per_err)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
